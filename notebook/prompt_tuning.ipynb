{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune Mistral 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Unsloth on your own computer, run the following below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "# %pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://github.com/unslothai/unsloth\n",
    "\n",
    "https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama#id-6.-alpaca-dataset\n",
    "\n",
    "https://www.youtube.com/watch?v=Gpyukc6c0w8\n",
    "\n",
    "https://huggingface.co/datasets/laion/OIG\n",
    "\n",
    "https://mer.vin/2024/02/unsloth-fine-tuning/\n",
    "\n",
    "RAG: https://www.youtube.com/watch?v=LKokLun3bHI\n",
    "\n",
    "https://huggingface.co/blog/synthetic-data-save-costs\n",
    "\n",
    "#### Load Model Through Unsloth API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "[2024-12-12 22:06:09,747] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Mistral patching. Transformers:4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA Graphics Device. Max memory: 15.702 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b411b65f11e4bb2a2bede3c7179eccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bd0c629e2e434187ef2dd4a1b65a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef82caaf5c04ee19cf61998084e90ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd726d93b0ea43be84d39701fe0631e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ca86c3ff3b437fae38fa751c539234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cabbf70a21d4c209c3f9ec3a228e855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7431ae03582545d38d2ab382592fd560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4acc06ab6984ec4acc754329711513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define format and function in order to format dataset,\n",
    "- It is supposed to include \"text\" keyword as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" [ {\"role\": \"user\", \"content\": {}}, {\"role\": \"assistant\", \"content\": {}}] \"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def format_prompts(batch):\n",
    "    answers         = batch[\"answer\"]\n",
    "    questions       = batch[\"question\"]\n",
    "    texts = []\n",
    "    for answer, question in zip(answers, questions):\n",
    "        conversation = []\n",
    "        conversation.append({\"role\": \"user\", \"content\": question})\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        chat = ','.join(str(x) for x in conversation) + EOS_TOKEN\n",
    "        texts.append(chat)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "def format_pandora_prompt(batch):\n",
    "    messages = batch[\"messages\"]\n",
    "    texts = []\n",
    "    for message in messages:\n",
    "        chat = ','.join(str(item) for item in message) + EOS_TOKEN\n",
    "        texts.append(chat)\n",
    "\n",
    "    return { \"text\" : texts, }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load dataset and format the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101114\n",
      "{'role': 'system', 'content': 'You are a helpful assistant with access to functions and tools. Use them if required.', 'tools': None, 'tool_calls': None, 'name': None},{'role': 'system', 'content': None, 'tools': '[{\"name\": \"check_email_availability\", \"description\": \"Check if an email address is available\", \"parameters\": {\"type\": \"object\", \"properties\": {\"email\": {\"type\": \"string\", \"description\": \"The email address to be checked\"}}, \"required\": [\"email\"]}}]', 'tool_calls': None, 'name': None},{'role': 'user', 'content': 'Hi, I want to create a new email account. Can you check if the email address \"johnsmith@gmail.com\" is available?', 'tools': None, 'tool_calls': None, 'name': None},{'role': 'assistant', 'content': None, 'tools': None, 'tool_calls': '[{\"name\": \"check_email_availability\", \"arguments\": {\"email\": \"johnsmith@gmail.com\"}}]', 'name': None},{'role': 'tool', 'content': '{\"status\": \"unavailable\", \"message\": \"The email address \\'johnsmith@gmail.com\\' is already in use.\"}', 'tools': None, 'tool_calls': None, 'name': 'check_email_availability'},{'role': 'assistant', 'content': \"I'm sorry, but the email address 'johnsmith@gmail.com' is already in use. You might want to try another one.\", 'tools': None, 'tool_calls': None, 'name': None},{'role': 'user', 'content': 'Okay, how about \"johnsmith123@gmail.com\"?', 'tools': None, 'tool_calls': None, 'name': None},{'role': 'assistant', 'content': None, 'tools': None, 'tool_calls': '[{\"name\": \"check_email_availability\", \"arguments\": {\"email\": \"johnsmith123@gmail.com\"}}]', 'name': None},{'role': 'tool', 'content': '{\"status\": \"available\", \"message\": \"The email address \\'johnsmith123@gmail.com\\' is available.\"}', 'tools': None, 'tool_calls': None, 'name': 'check_email_availability'},{'role': 'assistant', 'content': \"Good news! The email address 'johnsmith123@gmail.com' is available. You can use this for your new account.\", 'tools': None, 'tool_calls': None, 'name': None}</s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"danilopeixoto/pandora-tool-calling\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.map(format_pandora_prompt, batched=True)\n",
    "\n",
    "# Print a sample to verify\n",
    "print(str(len(dataset['text'])))\n",
    "print(dataset['text'][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import wandb to trace and log fine-tuning process,\n",
    "- import trainer and set training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimgesonmez90\u001b[0m (\u001b[33msimgesonmez90-i-zmir-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/simges/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/simges/autogen/autogen/notebook/wandb/run-20241211_194403-6x0pkhx0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/simgesonmez90-i-zmir-institute-of-technology/Fine%20tuning%20mistral%207B/runs/6x0pkhx0' target=\"_blank\">radiant-microwave-15</a></strong> to <a href='https://wandb.ai/simgesonmez90-i-zmir-institute-of-technology/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/simgesonmez90-i-zmir-institute-of-technology/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/simgesonmez90-i-zmir-institute-of-technology/Fine%20tuning%20mistral%207B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/simgesonmez90-i-zmir-institute-of-technology/Fine%20tuning%20mistral%207B/runs/6x0pkhx0' target=\"_blank\">https://wandb.ai/simgesonmez90-i-zmir-institute-of-technology/Fine%20tuning%20mistral%207B/runs/6x0pkhx0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key = \"286a64856911076c58502477a4541c62b0deb138\")\n",
    "run = wandb.init(\n",
    "    project='Fine tuning mistral nemo',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69c2dc1028648469c67f84ef0ce332d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/101114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import os,torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps=-1, # Set num_train_epochs = 1 for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(), # lower precision for adam optimizer, 32 bit is costly\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"tool_calling_llm\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA Graphics Device. Max memory = 15.702 GB.\n",
      "8.48 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 101,114 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 12,639\n",
      " \"-____-\"     Number of trainable parameters = 28,508,160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e66097197f44a02b4059cb1107ea3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3813, 'grad_norm': 0.21523892879486084, 'learning_rate': 0.00019928763653633054, 'epoch': 0.0}\n",
      "{'loss': 0.3045, 'grad_norm': 0.2527959644794464, 'learning_rate': 0.0001984961215766978, 'epoch': 0.01}\n",
      "{'loss': 0.3112, 'grad_norm': 0.20626042783260345, 'learning_rate': 0.00019770460661706508, 'epoch': 0.01}\n",
      "{'loss': 0.2972, 'grad_norm': 0.19636932015419006, 'learning_rate': 0.00019691309165743233, 'epoch': 0.02}\n",
      "{'loss': 0.322, 'grad_norm': 0.24558663368225098, 'learning_rate': 0.0001961215766977996, 'epoch': 0.02}\n",
      "{'loss': 0.2974, 'grad_norm': 0.21398065984249115, 'learning_rate': 0.00019533006173816687, 'epoch': 0.02}\n",
      "{'loss': 0.3067, 'grad_norm': 0.19154173135757446, 'learning_rate': 0.0001945385467785341, 'epoch': 0.03}\n",
      "{'loss': 0.2995, 'grad_norm': 0.24571380019187927, 'learning_rate': 0.00019374703181890138, 'epoch': 0.03}\n",
      "{'loss': 0.2776, 'grad_norm': 0.22519053518772125, 'learning_rate': 0.00019295551685926865, 'epoch': 0.04}\n",
      "{'loss': 0.2799, 'grad_norm': 0.2156495302915573, 'learning_rate': 0.0001921640018996359, 'epoch': 0.04}\n",
      "{'loss': 0.3033, 'grad_norm': 0.23191452026367188, 'learning_rate': 0.0001913724869400032, 'epoch': 0.04}\n",
      "{'loss': 0.3063, 'grad_norm': 0.15418513119220734, 'learning_rate': 0.00019058097198037044, 'epoch': 0.05}\n",
      "{'loss': 0.2845, 'grad_norm': 0.2513321042060852, 'learning_rate': 0.00018978945702073768, 'epoch': 0.05}\n",
      "{'loss': 0.26, 'grad_norm': 0.26366350054740906, 'learning_rate': 0.00018899794206110498, 'epoch': 0.06}\n",
      "{'loss': 0.312, 'grad_norm': 0.25146931409835815, 'learning_rate': 0.00018820642710147222, 'epoch': 0.06}\n",
      "{'loss': 0.277, 'grad_norm': 0.23971830308437347, 'learning_rate': 0.0001874149121418395, 'epoch': 0.06}\n",
      "{'loss': 0.2949, 'grad_norm': 0.24213124811649323, 'learning_rate': 0.00018662339718220677, 'epoch': 0.07}\n",
      "{'loss': 0.2893, 'grad_norm': 0.6596011519432068, 'learning_rate': 0.000185831882222574, 'epoch': 0.07}\n",
      "{'loss': 0.2706, 'grad_norm': 0.21991266310214996, 'learning_rate': 0.00018504036726294128, 'epoch': 0.08}\n",
      "{'loss': 0.2868, 'grad_norm': 0.1925252079963684, 'learning_rate': 0.00018424885230330855, 'epoch': 0.08}\n",
      "{'loss': 0.264, 'grad_norm': 0.2338474988937378, 'learning_rate': 0.0001834573373436758, 'epoch': 0.08}\n",
      "{'loss': 0.3051, 'grad_norm': 0.21254940330982208, 'learning_rate': 0.00018266582238404307, 'epoch': 0.09}\n",
      "{'loss': 0.3055, 'grad_norm': 0.2966887354850769, 'learning_rate': 0.00018187430742441034, 'epoch': 0.09}\n",
      "{'loss': 0.2796, 'grad_norm': 0.40462252497673035, 'learning_rate': 0.0001810827924647776, 'epoch': 0.09}\n",
      "{'loss': 0.2909, 'grad_norm': 0.20106351375579834, 'learning_rate': 0.00018029127750514485, 'epoch': 0.1}\n",
      "{'loss': 0.2848, 'grad_norm': 0.2353917956352234, 'learning_rate': 0.00017949976254551212, 'epoch': 0.1}\n",
      "{'loss': 0.2787, 'grad_norm': 0.2627285420894623, 'learning_rate': 0.0001787082475858794, 'epoch': 0.11}\n",
      "{'loss': 0.2842, 'grad_norm': 0.3224586546421051, 'learning_rate': 0.00017791673262624664, 'epoch': 0.11}\n",
      "{'loss': 0.2585, 'grad_norm': 0.20512722432613373, 'learning_rate': 0.0001771252176666139, 'epoch': 0.11}\n",
      "{'loss': 0.2773, 'grad_norm': 0.23089411854743958, 'learning_rate': 0.00017633370270698118, 'epoch': 0.12}\n",
      "{'loss': 0.2686, 'grad_norm': 0.21093906462192535, 'learning_rate': 0.00017554218774734842, 'epoch': 0.12}\n",
      "{'loss': 0.2735, 'grad_norm': 0.2235269397497177, 'learning_rate': 0.0001747506727877157, 'epoch': 0.13}\n",
      "{'loss': 0.2827, 'grad_norm': 0.2027398645877838, 'learning_rate': 0.00017395915782808297, 'epoch': 0.13}\n",
      "{'loss': 0.2508, 'grad_norm': 0.2170359343290329, 'learning_rate': 0.0001731676428684502, 'epoch': 0.13}\n",
      "{'loss': 0.2837, 'grad_norm': 0.2321755737066269, 'learning_rate': 0.00017237612790881748, 'epoch': 0.14}\n",
      "{'loss': 0.2785, 'grad_norm': 0.21116647124290466, 'learning_rate': 0.00017158461294918475, 'epoch': 0.14}\n",
      "{'loss': 0.279, 'grad_norm': 0.2035714089870453, 'learning_rate': 0.00017079309798955202, 'epoch': 0.15}\n",
      "{'loss': 0.2797, 'grad_norm': 0.25753429532051086, 'learning_rate': 0.00017000158302991927, 'epoch': 0.15}\n",
      "{'loss': 0.2758, 'grad_norm': 0.1967354416847229, 'learning_rate': 0.00016921006807028654, 'epoch': 0.15}\n",
      "{'loss': 0.2682, 'grad_norm': 0.2179282307624817, 'learning_rate': 0.0001684185531106538, 'epoch': 0.16}\n",
      "{'loss': 0.2753, 'grad_norm': 0.24109618365764618, 'learning_rate': 0.00016762703815102105, 'epoch': 0.16}\n",
      "{'loss': 0.2604, 'grad_norm': 0.28082388639450073, 'learning_rate': 0.00016683552319138832, 'epoch': 0.17}\n",
      "{'loss': 0.2849, 'grad_norm': 0.22482137382030487, 'learning_rate': 0.0001660440082317556, 'epoch': 0.17}\n",
      "{'loss': 0.2684, 'grad_norm': 0.17248904705047607, 'learning_rate': 0.00016525249327212284, 'epoch': 0.17}\n",
      "{'loss': 0.2745, 'grad_norm': 0.23030725121498108, 'learning_rate': 0.0001644609783124901, 'epoch': 0.18}\n",
      "{'loss': 0.2617, 'grad_norm': 0.2017725706100464, 'learning_rate': 0.00016366946335285738, 'epoch': 0.18}\n",
      "{'loss': 0.3035, 'grad_norm': 0.2427261918783188, 'learning_rate': 0.00016287794839322462, 'epoch': 0.19}\n",
      "{'loss': 0.2773, 'grad_norm': 0.2377670258283615, 'learning_rate': 0.00016208643343359192, 'epoch': 0.19}\n",
      "{'loss': 0.2973, 'grad_norm': 0.23023928701877594, 'learning_rate': 0.00016129491847395916, 'epoch': 0.19}\n",
      "{'loss': 0.2582, 'grad_norm': 0.2380305528640747, 'learning_rate': 0.00016050340351432644, 'epoch': 0.2}\n",
      "{'loss': 0.2606, 'grad_norm': 0.24151018261909485, 'learning_rate': 0.0001597118885546937, 'epoch': 0.2}\n",
      "{'loss': 0.2731, 'grad_norm': 0.16021619737148285, 'learning_rate': 0.00015892037359506095, 'epoch': 0.21}\n",
      "{'loss': 0.2894, 'grad_norm': 0.30257734656333923, 'learning_rate': 0.00015812885863542822, 'epoch': 0.21}\n",
      "{'loss': 0.2802, 'grad_norm': 0.21787531673908234, 'learning_rate': 0.0001573373436757955, 'epoch': 0.21}\n",
      "{'loss': 0.2716, 'grad_norm': 0.22880281507968903, 'learning_rate': 0.00015654582871616274, 'epoch': 0.22}\n",
      "{'loss': 0.2772, 'grad_norm': 0.24514387547969818, 'learning_rate': 0.00015575431375653, 'epoch': 0.22}\n",
      "{'loss': 0.2898, 'grad_norm': 0.22648687660694122, 'learning_rate': 0.00015496279879689728, 'epoch': 0.23}\n",
      "{'loss': 0.2658, 'grad_norm': 0.21240633726119995, 'learning_rate': 0.00015417128383726452, 'epoch': 0.23}\n",
      "{'loss': 0.2586, 'grad_norm': 0.27618473768234253, 'learning_rate': 0.0001533797688776318, 'epoch': 0.23}\n",
      "{'loss': 0.2765, 'grad_norm': 0.12895961105823517, 'learning_rate': 0.00015258825391799904, 'epoch': 0.24}\n",
      "{'loss': 0.2749, 'grad_norm': 0.25192567706108093, 'learning_rate': 0.00015179673895836633, 'epoch': 0.24}\n",
      "{'loss': 0.2781, 'grad_norm': 0.211591899394989, 'learning_rate': 0.00015100522399873358, 'epoch': 0.25}\n",
      "{'loss': 0.2423, 'grad_norm': 0.2972756624221802, 'learning_rate': 0.00015021370903910085, 'epoch': 0.25}\n",
      "{'loss': 0.2704, 'grad_norm': 0.26308995485305786, 'learning_rate': 0.00014942219407946812, 'epoch': 0.25}\n",
      "{'loss': 0.2704, 'grad_norm': 0.15108560025691986, 'learning_rate': 0.00014863067911983536, 'epoch': 0.26}\n",
      "{'loss': 0.2818, 'grad_norm': 0.24649319052696228, 'learning_rate': 0.00014783916416020264, 'epoch': 0.26}\n",
      "{'loss': 0.2855, 'grad_norm': 0.2507116496562958, 'learning_rate': 0.0001470476492005699, 'epoch': 0.27}\n",
      "{'loss': 0.247, 'grad_norm': 0.1688561588525772, 'learning_rate': 0.00014625613424093715, 'epoch': 0.27}\n",
      "{'loss': 0.2695, 'grad_norm': 0.24367856979370117, 'learning_rate': 0.00014546461928130442, 'epoch': 0.27}\n",
      "{'loss': 0.2915, 'grad_norm': 0.2517593204975128, 'learning_rate': 0.0001446731043216717, 'epoch': 0.28}\n",
      "{'loss': 0.2458, 'grad_norm': 0.21562573313713074, 'learning_rate': 0.00014388158936203894, 'epoch': 0.28}\n",
      "{'loss': 0.3025, 'grad_norm': 0.23181432485580444, 'learning_rate': 0.0001430900744024062, 'epoch': 0.28}\n",
      "{'loss': 0.2442, 'grad_norm': 0.15961439907550812, 'learning_rate': 0.00014229855944277348, 'epoch': 0.29}\n",
      "{'loss': 0.2546, 'grad_norm': 0.27672311663627625, 'learning_rate': 0.00014150704448314075, 'epoch': 0.29}\n",
      "{'loss': 0.2594, 'grad_norm': 0.22393332421779633, 'learning_rate': 0.000140715529523508, 'epoch': 0.3}\n",
      "{'loss': 0.2742, 'grad_norm': 0.2205943912267685, 'learning_rate': 0.00013992401456387526, 'epoch': 0.3}\n",
      "{'loss': 0.2912, 'grad_norm': 0.2136358618736267, 'learning_rate': 0.00013913249960424253, 'epoch': 0.3}\n",
      "{'loss': 0.275, 'grad_norm': 0.262149840593338, 'learning_rate': 0.00013834098464460978, 'epoch': 0.31}\n",
      "{'loss': 0.2673, 'grad_norm': 0.2766246199607849, 'learning_rate': 0.00013754946968497705, 'epoch': 0.31}\n",
      "{'loss': 0.2497, 'grad_norm': 0.26423969864845276, 'learning_rate': 0.00013675795472534432, 'epoch': 0.32}\n",
      "{'loss': 0.2588, 'grad_norm': 0.26714026927948, 'learning_rate': 0.00013596643976571156, 'epoch': 0.32}\n",
      "{'loss': 0.2722, 'grad_norm': 0.2645283341407776, 'learning_rate': 0.00013517492480607886, 'epoch': 0.32}\n",
      "{'loss': 0.2635, 'grad_norm': 0.22511227428913116, 'learning_rate': 0.0001343834098464461, 'epoch': 0.33}\n",
      "{'loss': 0.2429, 'grad_norm': 0.2657678425312042, 'learning_rate': 0.00013359189488681335, 'epoch': 0.33}\n",
      "{'loss': 0.2691, 'grad_norm': 0.27370181679725647, 'learning_rate': 0.00013280037992718065, 'epoch': 0.34}\n",
      "{'loss': 0.2659, 'grad_norm': 0.228836327791214, 'learning_rate': 0.0001320088649675479, 'epoch': 0.34}\n",
      "{'loss': 0.2864, 'grad_norm': 0.259837806224823, 'learning_rate': 0.00013121735000791516, 'epoch': 0.34}\n",
      "{'loss': 0.2609, 'grad_norm': 0.2267512083053589, 'learning_rate': 0.00013042583504828243, 'epoch': 0.35}\n",
      "{'loss': 0.2819, 'grad_norm': 0.21836763620376587, 'learning_rate': 0.00012963432008864968, 'epoch': 0.35}\n",
      "{'loss': 0.272, 'grad_norm': 0.1653199940919876, 'learning_rate': 0.00012884280512901695, 'epoch': 0.36}\n",
      "{'loss': 0.2817, 'grad_norm': 0.2431064248085022, 'learning_rate': 0.0001280512901693842, 'epoch': 0.36}\n",
      "{'loss': 0.2666, 'grad_norm': 0.1707567423582077, 'learning_rate': 0.00012725977520975146, 'epoch': 0.36}\n",
      "{'loss': 0.2617, 'grad_norm': 0.2522309720516205, 'learning_rate': 0.00012646826025011873, 'epoch': 0.37}\n",
      "{'loss': 0.2874, 'grad_norm': 0.2895015478134155, 'learning_rate': 0.00012567674529048598, 'epoch': 0.37}\n",
      "{'loss': 0.2923, 'grad_norm': 0.27253419160842896, 'learning_rate': 0.00012488523033085328, 'epoch': 0.38}\n",
      "{'loss': 0.2597, 'grad_norm': 0.22548508644104004, 'learning_rate': 0.00012409371537122052, 'epoch': 0.38}\n",
      "{'loss': 0.2603, 'grad_norm': 0.24070462584495544, 'learning_rate': 0.00012330220041158776, 'epoch': 0.38}\n",
      "{'loss': 0.2446, 'grad_norm': 0.24559935927391052, 'learning_rate': 0.00012251068545195506, 'epoch': 0.39}\n",
      "{'loss': 0.2871, 'grad_norm': 0.22163893282413483, 'learning_rate': 0.0001217191704923223, 'epoch': 0.39}\n",
      "{'loss': 0.2673, 'grad_norm': 0.18264910578727722, 'learning_rate': 0.00012092765553268956, 'epoch': 0.4}\n",
      "{'loss': 0.2419, 'grad_norm': 0.25341880321502686, 'learning_rate': 0.00012013614057305685, 'epoch': 0.4}\n",
      "{'loss': 0.2606, 'grad_norm': 0.1458718478679657, 'learning_rate': 0.0001193446256134241, 'epoch': 0.4}\n",
      "{'loss': 0.256, 'grad_norm': 0.25840240716934204, 'learning_rate': 0.00011855311065379136, 'epoch': 0.41}\n",
      "{'loss': 0.2778, 'grad_norm': 0.22644418478012085, 'learning_rate': 0.00011776159569415863, 'epoch': 0.41}\n",
      "{'loss': 0.2746, 'grad_norm': 0.2765261232852936, 'learning_rate': 0.00011697008073452589, 'epoch': 0.42}\n",
      "{'loss': 0.2647, 'grad_norm': 0.23068584501743317, 'learning_rate': 0.00011617856577489315, 'epoch': 0.42}\n",
      "{'loss': 0.2659, 'grad_norm': 0.24292893707752228, 'learning_rate': 0.00011538705081526042, 'epoch': 0.42}\n",
      "{'loss': 0.2651, 'grad_norm': 0.24145877361297607, 'learning_rate': 0.00011459553585562768, 'epoch': 0.43}\n",
      "{'loss': 0.2495, 'grad_norm': 0.24650923907756805, 'learning_rate': 0.00011380402089599493, 'epoch': 0.43}\n",
      "{'loss': 0.287, 'grad_norm': 0.20496892929077148, 'learning_rate': 0.00011301250593636222, 'epoch': 0.44}\n",
      "{'loss': 0.2617, 'grad_norm': 0.24813826382160187, 'learning_rate': 0.00011222099097672946, 'epoch': 0.44}\n",
      "{'loss': 0.262, 'grad_norm': 0.25784292817115784, 'learning_rate': 0.00011142947601709672, 'epoch': 0.44}\n",
      "{'loss': 0.2719, 'grad_norm': 0.2669885456562042, 'learning_rate': 0.000110637961057464, 'epoch': 0.45}\n",
      "{'loss': 0.2574, 'grad_norm': 0.15019261837005615, 'learning_rate': 0.00010984644609783126, 'epoch': 0.45}\n",
      "{'loss': 0.2769, 'grad_norm': 0.21500639617443085, 'learning_rate': 0.00010905493113819852, 'epoch': 0.45}\n",
      "{'loss': 0.2775, 'grad_norm': 0.27688491344451904, 'learning_rate': 0.00010826341617856579, 'epoch': 0.46}\n",
      "{'loss': 0.2308, 'grad_norm': 0.29263439774513245, 'learning_rate': 0.00010747190121893305, 'epoch': 0.46}\n",
      "{'loss': 0.2485, 'grad_norm': 0.32166168093681335, 'learning_rate': 0.0001066803862593003, 'epoch': 0.47}\n",
      "{'loss': 0.2856, 'grad_norm': 0.26029956340789795, 'learning_rate': 0.00010588887129966756, 'epoch': 0.47}\n",
      "{'loss': 0.2495, 'grad_norm': 0.2719380259513855, 'learning_rate': 0.00010509735634003483, 'epoch': 0.47}\n",
      "{'loss': 0.2824, 'grad_norm': 0.2170204371213913, 'learning_rate': 0.00010430584138040209, 'epoch': 0.48}\n",
      "{'loss': 0.2576, 'grad_norm': 0.25960224866867065, 'learning_rate': 0.00010351432642076935, 'epoch': 0.48}\n",
      "{'loss': 0.2617, 'grad_norm': 0.24471813440322876, 'learning_rate': 0.00010272281146113663, 'epoch': 0.49}\n",
      "{'loss': 0.2695, 'grad_norm': 0.28214403986930847, 'learning_rate': 0.00010193129650150387, 'epoch': 0.49}\n",
      "{'loss': 0.2817, 'grad_norm': 0.23973681032657623, 'learning_rate': 0.00010113978154187113, 'epoch': 0.49}\n",
      "{'loss': 0.2615, 'grad_norm': 0.2655722200870514, 'learning_rate': 0.00010034826658223842, 'epoch': 0.5}\n",
      "{'loss': 0.2861, 'grad_norm': 0.16746562719345093, 'learning_rate': 9.955675162260567e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2613, 'grad_norm': 0.24176320433616638, 'learning_rate': 9.876523666297293e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2568, 'grad_norm': 0.23463645577430725, 'learning_rate': 9.797372170334019e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2463, 'grad_norm': 0.1892179548740387, 'learning_rate': 9.718220674370746e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2764, 'grad_norm': 0.24245527386665344, 'learning_rate': 9.639069178407473e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2564, 'grad_norm': 0.22375057637691498, 'learning_rate': 9.559917682444199e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2437, 'grad_norm': 0.22012795507907867, 'learning_rate': 9.480766186480925e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2645, 'grad_norm': 0.21300029754638672, 'learning_rate': 9.401614690517652e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2749, 'grad_norm': 0.2872764468193054, 'learning_rate': 9.322463194554377e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2648, 'grad_norm': 0.21175871789455414, 'learning_rate': 9.243311698591104e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2627, 'grad_norm': 0.24419258534908295, 'learning_rate': 9.16416020262783e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2634, 'grad_norm': 0.21662084758281708, 'learning_rate': 9.085008706664556e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2869, 'grad_norm': 0.2619873583316803, 'learning_rate': 9.005857210701283e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2742, 'grad_norm': 0.27478358149528503, 'learning_rate': 8.926705714738009e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2688, 'grad_norm': 0.24596160650253296, 'learning_rate': 8.847554218774734e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2567, 'grad_norm': 0.18540138006210327, 'learning_rate': 8.768402722811462e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2627, 'grad_norm': 0.2391219586133957, 'learning_rate': 8.689251226848187e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2743, 'grad_norm': 0.2637728750705719, 'learning_rate': 8.610099730884914e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2643, 'grad_norm': 0.20580632984638214, 'learning_rate': 8.53094823492164e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2341, 'grad_norm': 0.19514892995357513, 'learning_rate': 8.451796738958366e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2617, 'grad_norm': 0.25921201705932617, 'learning_rate': 8.372645242995093e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2358, 'grad_norm': 0.3374575078487396, 'learning_rate': 8.29349374703182e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2637, 'grad_norm': 0.2170558124780655, 'learning_rate': 8.214342251068546e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2769, 'grad_norm': 0.28256505727767944, 'learning_rate': 8.135190755105272e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2714, 'grad_norm': 0.2766828238964081, 'learning_rate': 8.056039259141999e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2667, 'grad_norm': 0.3690130412578583, 'learning_rate': 7.976887763178724e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2727, 'grad_norm': 0.25938108563423157, 'learning_rate': 7.897736267215451e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2735, 'grad_norm': 0.2624664008617401, 'learning_rate': 7.818584771252177e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2843, 'grad_norm': 0.18183861672878265, 'learning_rate': 7.739433275288903e-05, 'epoch': 0.61}\n",
      "{'loss': 0.263, 'grad_norm': 0.23164218664169312, 'learning_rate': 7.66028177932563e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2478, 'grad_norm': 0.2368878871202469, 'learning_rate': 7.581130283362356e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2843, 'grad_norm': 0.3710848391056061, 'learning_rate': 7.501978787399082e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2702, 'grad_norm': 0.21493370831012726, 'learning_rate': 7.422827291435809e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2564, 'grad_norm': 0.20666854083538055, 'learning_rate': 7.343675795472534e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2572, 'grad_norm': 0.26301881670951843, 'learning_rate': 7.264524299509261e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2621, 'grad_norm': 0.2448103278875351, 'learning_rate': 7.185372803545987e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2509, 'grad_norm': 0.25996825098991394, 'learning_rate': 7.106221307582713e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2458, 'grad_norm': 0.26147332787513733, 'learning_rate': 7.02706981161944e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2714, 'grad_norm': 0.25773805379867554, 'learning_rate': 6.947918315656167e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2706, 'grad_norm': 0.2803058624267578, 'learning_rate': 6.868766819692893e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2486, 'grad_norm': 0.23014122247695923, 'learning_rate': 6.789615323729619e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2692, 'grad_norm': 0.18305452167987823, 'learning_rate': 6.710463827766346e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2619, 'grad_norm': 0.23032090067863464, 'learning_rate': 6.631312331803071e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2406, 'grad_norm': 0.21984802186489105, 'learning_rate': 6.552160835839798e-05, 'epoch': 0.67}\n",
      "{'loss': 0.243, 'grad_norm': 0.20196865499019623, 'learning_rate': 6.473009339876523e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2513, 'grad_norm': 0.21763192117214203, 'learning_rate': 6.39385784391325e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2659, 'grad_norm': 0.2118932604789734, 'learning_rate': 6.314706347949977e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2474, 'grad_norm': 0.23440134525299072, 'learning_rate': 6.235554851986703e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2418, 'grad_norm': 0.2870267331600189, 'learning_rate': 6.156403356023429e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2474, 'grad_norm': 0.22311332821846008, 'learning_rate': 6.0772518600601556e-05, 'epoch': 0.7}\n",
      "{'loss': 0.238, 'grad_norm': 0.22501377761363983, 'learning_rate': 5.9981003640968814e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2667, 'grad_norm': 0.24650172889232635, 'learning_rate': 5.918948868133608e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2592, 'grad_norm': 0.14170025289058685, 'learning_rate': 5.839797372170335e-05, 'epoch': 0.71}\n",
      "{'loss': 0.255, 'grad_norm': 0.25094228982925415, 'learning_rate': 5.7606458762070606e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2662, 'grad_norm': 0.16882099211215973, 'learning_rate': 5.681494380243787e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2371, 'grad_norm': 0.21213726699352264, 'learning_rate': 5.6023428842805134e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2677, 'grad_norm': 0.2599644064903259, 'learning_rate': 5.523191388317239e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2619, 'grad_norm': 0.20819686353206635, 'learning_rate': 5.444039892353966e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2401, 'grad_norm': 0.2367434799671173, 'learning_rate': 5.364888396390691e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2574, 'grad_norm': 0.2520817518234253, 'learning_rate': 5.2857369004274184e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2465, 'grad_norm': 0.23186703026294708, 'learning_rate': 5.206585404464145e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2564, 'grad_norm': 0.17664019763469696, 'learning_rate': 5.1274339085008706e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2581, 'grad_norm': 0.24880275130271912, 'learning_rate': 5.048282412537597e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2458, 'grad_norm': 0.16378697752952576, 'learning_rate': 4.9691309165743234e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2627, 'grad_norm': 0.3094651401042938, 'learning_rate': 4.88997942061105e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2579, 'grad_norm': 0.1382744014263153, 'learning_rate': 4.810827924647776e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2495, 'grad_norm': 0.2276732474565506, 'learning_rate': 4.731676428684502e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2482, 'grad_norm': 0.22216956317424774, 'learning_rate': 4.652524932721229e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2584, 'grad_norm': 0.5992056727409363, 'learning_rate': 4.573373436757955e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2462, 'grad_norm': 0.2356559932231903, 'learning_rate': 4.494221940794681e-05, 'epoch': 0.78}\n",
      "{'loss': 0.256, 'grad_norm': 0.24050992727279663, 'learning_rate': 4.4150704448314076e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2549, 'grad_norm': 0.23101955652236938, 'learning_rate': 4.335918948868134e-05, 'epoch': 0.78}\n",
      "{'loss': 0.227, 'grad_norm': 0.2500928044319153, 'learning_rate': 4.2567674529048605e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2844, 'grad_norm': 0.28495049476623535, 'learning_rate': 4.177615956941586e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2357, 'grad_norm': 0.11417096853256226, 'learning_rate': 4.0984644609783126e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2621, 'grad_norm': 0.27495020627975464, 'learning_rate': 4.019312965015039e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2348, 'grad_norm': 0.24813149869441986, 'learning_rate': 3.9401614690517654e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2674, 'grad_norm': 0.16459126770496368, 'learning_rate': 3.861009973088491e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2265, 'grad_norm': 0.2755063474178314, 'learning_rate': 3.781858477125218e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2304, 'grad_norm': 0.2385675311088562, 'learning_rate': 3.702706981161944e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2332, 'grad_norm': 0.2594110369682312, 'learning_rate': 3.6235554851986704e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2506, 'grad_norm': 0.24773673713207245, 'learning_rate': 3.544403989235397e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2616, 'grad_norm': 0.17500057816505432, 'learning_rate': 3.465252493272123e-05, 'epoch': 0.83}\n",
      "{'loss': 0.249, 'grad_norm': 0.22136420011520386, 'learning_rate': 3.386100997308849e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2505, 'grad_norm': 0.2576604187488556, 'learning_rate': 3.3069495013455754e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2582, 'grad_norm': 0.2765847444534302, 'learning_rate': 3.227798005382302e-05, 'epoch': 0.84}\n",
      "{'loss': 0.2562, 'grad_norm': 0.29997870326042175, 'learning_rate': 3.148646509419028e-05, 'epoch': 0.84}\n",
      "{'loss': 0.2483, 'grad_norm': 0.3098241686820984, 'learning_rate': 3.069495013455754e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2601, 'grad_norm': 0.2408013641834259, 'learning_rate': 2.990343517492481e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2572, 'grad_norm': 0.2400466948747635, 'learning_rate': 2.911192021529207e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2558, 'grad_norm': 0.16618824005126953, 'learning_rate': 2.8320405255659332e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2767, 'grad_norm': 0.228347510099411, 'learning_rate': 2.7528890296026593e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2451, 'grad_norm': 0.11807138472795486, 'learning_rate': 2.673737533639386e-05, 'epoch': 0.87}\n",
      "{'loss': 0.2181, 'grad_norm': 0.19888418912887573, 'learning_rate': 2.594586037676112e-05, 'epoch': 0.87}\n",
      "{'loss': 0.2595, 'grad_norm': 0.25941047072410583, 'learning_rate': 2.5154345417128382e-05, 'epoch': 0.87}\n",
      "{'loss': 0.2559, 'grad_norm': 0.22702987492084503, 'learning_rate': 2.4362830457495646e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2563, 'grad_norm': 0.2446211874485016, 'learning_rate': 2.357131549786291e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2632, 'grad_norm': 0.2489149421453476, 'learning_rate': 2.277980053823017e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2331, 'grad_norm': 0.1778663694858551, 'learning_rate': 2.1988285578597435e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2384, 'grad_norm': 0.29049745202064514, 'learning_rate': 2.11967706189647e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2572, 'grad_norm': 0.2579682469367981, 'learning_rate': 2.0405255659331964e-05, 'epoch': 0.9}\n",
      "{'loss': 0.2579, 'grad_norm': 0.22947192192077637, 'learning_rate': 1.9613740699699228e-05, 'epoch': 0.9}\n",
      "{'loss': 0.2326, 'grad_norm': 0.24083155393600464, 'learning_rate': 1.882222574006649e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2565, 'grad_norm': 0.26480770111083984, 'learning_rate': 1.8030710780433753e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2546, 'grad_norm': 0.228322833776474, 'learning_rate': 1.7239195820801013e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2652, 'grad_norm': 0.1475362628698349, 'learning_rate': 1.6447680861168278e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2483, 'grad_norm': 0.21228504180908203, 'learning_rate': 1.565616590153554e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2573, 'grad_norm': 0.2579675614833832, 'learning_rate': 1.4864650941902802e-05, 'epoch': 0.93}\n",
      "{'loss': 0.2457, 'grad_norm': 0.27801433205604553, 'learning_rate': 1.4073135982270067e-05, 'epoch': 0.93}\n",
      "{'loss': 0.2595, 'grad_norm': 0.2679731547832489, 'learning_rate': 1.3281621022637327e-05, 'epoch': 0.93}\n",
      "{'loss': 0.2611, 'grad_norm': 0.28034788370132446, 'learning_rate': 1.2490106063004591e-05, 'epoch': 0.94}\n",
      "{'loss': 0.275, 'grad_norm': 0.25365960597991943, 'learning_rate': 1.1698591103371854e-05, 'epoch': 0.94}\n",
      "{'loss': 0.263, 'grad_norm': 0.23569484055042267, 'learning_rate': 1.0907076143739116e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2487, 'grad_norm': 0.32269537448883057, 'learning_rate': 1.011556118410638e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2572, 'grad_norm': 0.25767993927001953, 'learning_rate': 9.324046224473643e-06, 'epoch': 0.95}\n",
      "{'loss': 0.2628, 'grad_norm': 0.25143763422966003, 'learning_rate': 8.532531264840905e-06, 'epoch': 0.96}\n",
      "{'loss': 0.2463, 'grad_norm': 0.30251839756965637, 'learning_rate': 7.74101630520817e-06, 'epoch': 0.96}\n",
      "{'loss': 0.2464, 'grad_norm': 0.26142746210098267, 'learning_rate': 6.949501345575432e-06, 'epoch': 0.97}\n",
      "{'loss': 0.2589, 'grad_norm': 0.23399263620376587, 'learning_rate': 6.1579863859426945e-06, 'epoch': 0.97}\n",
      "{'loss': 0.2772, 'grad_norm': 0.2525801956653595, 'learning_rate': 5.366471426309958e-06, 'epoch': 0.97}\n",
      "{'loss': 0.2505, 'grad_norm': 0.2296481430530548, 'learning_rate': 4.57495646667722e-06, 'epoch': 0.98}\n",
      "{'loss': 0.2542, 'grad_norm': 0.27229398488998413, 'learning_rate': 3.7834415070444836e-06, 'epoch': 0.98}\n",
      "{'loss': 0.2303, 'grad_norm': 0.2488405704498291, 'learning_rate': 2.991926547411746e-06, 'epoch': 0.99}\n",
      "{'loss': 0.2576, 'grad_norm': 0.2158137410879135, 'learning_rate': 2.2004115877790094e-06, 'epoch': 0.99}\n",
      "{'loss': 0.2572, 'grad_norm': 0.17482155561447144, 'learning_rate': 1.408896628146272e-06, 'epoch': 0.99}\n",
      "{'loss': 0.248, 'grad_norm': 0.250389039516449, 'learning_rate': 6.173816685135349e-07, 'epoch': 1.0}\n",
      "{'train_runtime': 99647.716, 'train_samples_per_second': 1.015, 'train_steps_per_second': 0.127, 'train_loss': 0.2657410006459373, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99647.716 seconds used for training.\n",
      "1660.8 minutes used for training.\n",
      "Peak reserved memory = 11.854 GB.\n",
      "Peak reserved memory for training = 3.374 GB.\n",
      "Peak reserved memory % of max memory = 75.494 %.\n",
      "Peak reserved memory for training % of max memory = 21.488 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 78.38 out of 125.63 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 5/40 [00:00<00:01, 20.41it/s]We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:14<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"Mistral-Nemo-Instruct-2407\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5c1e01ccf34bd6b8a522e48007d4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login() # 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-675d623c-0b27184276bc7eb43b936526;f44683fd-19ed-4f9d-b9ba-b70656a479d8)\n\nRepository Not Found for url: https://huggingface.co/api/repos/delete.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/api/repos/delete",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimges/Mistral-7B-Instruct-v0.3-Lora-Adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3596\u001b[0m, in \u001b[0;36mHfApi.delete_repo\u001b[0;34m(self, repo_id, token, repo_type, missing_ok)\u001b[0m\n\u001b[1;32m   3594\u001b[0m r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mdelete(path, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mjson)\n\u001b[1;32m   3595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3596\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m   3598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_ok:\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepoNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    436\u001b[0m     response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     )\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-675d623c-0b27184276bc7eb43b936526;f44683fd-19ed-4f9d-b9ba-b70656a479d8)\n\nRepository Not Found for url: https://huggingface.co/api/repos/delete.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated."
     ]
    }
   ],
   "source": [
    "\n",
    "repo_id = \"simges/Mistral-Nemo-Instruct-2407\"\n",
    "api.delete_repo(repo_id=repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/simges/Mistral-Nemo-Instruct-2407', endpoint='https://huggingface.co', repo_type='model', repo_id='simges/Mistral-Nemo-Instruct-2407')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "repo_id = \"simges/Mistral-Nemo-Instruct-2407\"\n",
    "api.create_repo(repo_id=repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade --force-reinstall --no-cache-dir git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = simges.\n",
      "We shall truncate simges/Mistral-7B-Instruct-v0.3-Lora-Adapter to Mistral-7B-Instruct-v0.3-Lora-Adapter\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:16<00:00,  2.47it/s]\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-675d6259-6b178a11176caced35253cc3;8dd11e8b-0ab5-46c5-a03b-7b8a3bd8e9d3)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub_merged\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_16bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/unsloth/save.py:1343\u001b[0m, in \u001b[0;36munsloth_push_to_hub_merged\u001b[0;34m(self, repo_id, tokenizer, save_method, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1343\u001b[0m \u001b[43munsloth_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   1345\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/unsloth/save.py:663\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    660\u001b[0m old_padding_side \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side\n\u001b[1;32m    661\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 663\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_save_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# Revert back padding side\u001b[39;00m\n\u001b[1;32m    666\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m old_padding_side\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2573\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.save_pretrained\u001b[0;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m     commit_message \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit_message\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2572\u001b[0m     repo_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_directory\u001b[38;5;241m.\u001b[39msplit(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m-> 2573\u001b[0m     repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m     files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files_timestamps(save_directory)\n\u001b[1;32m   2576\u001b[0m special_tokens_map_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   2577\u001b[0m     save_directory, (filename_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename_prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m SPECIAL_TOKENS_MAP_FILE\n\u001b[1;32m   2578\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/transformers/utils/hub.py:740\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[0;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[1;32m    737\u001b[0m             repo_id \u001b[38;5;241m=\u001b[39m repo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    738\u001b[0m         repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 740\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\u001b[38;5;241m.\u001b[39mrepo_id\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3531\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3531\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3534\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/fine-tune/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-675d6259-6b178a11176caced35253cc3;8dd11e8b-0ab5-46c5-a03b-7b8a3bd8e9d3)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "\n",
    "model.push_to_hub_merged(repo_id, tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
